#!/usr/bin/python2.7
#discover urls in the domain by extract the href link in the content and crawl recursively to get all urls

import sys
import os
import requests
import re
import urllib.parse

class DiscoverUrlsInDomain:
  def __init__(self) -> None:
    try:
      self.TargetLinks = []

    except Exception as e:
      exc_type, exc_obj, exc_tb = sys.exc_info()
      fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
      print(exc_type, fname, exc_tb.tb_lineno)
      print(exc_obj)

  def extractLinksFrom(self, url):
    try:
      response = requests.get(url)
      return re.findall('(?:href=")(.*?)"', response.content)

    except Exception as e:
      exc_type, exc_obj, exc_tb = sys.exc_info()
      fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
      print(exc_type, fname, exc_tb.tb_lineno)
      print(exc_obj)

  def crawl(self, url):
    try:
      href_links = self.extractLinksFrom(url)

      for link in href_links:
        link = urlparse.urljoin(url, link)

        if "#" in link:
          link = link.split("#")[0]

        if url in link and link not in self.TargetLinks:
          print(link)
          self.crawl(link)

    except Exception as e:
      exc_type, exc_obj, exc_tb = sys.exc_info()
      fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
      print(exc_type, fname, exc_tb.tb_lineno)
      print(exc_obj)

if __name__ == "__main__":
  p = DiscoverUrlsInDomain()
  p.crawl("https://stackoverflow.com/questions/48072619/how-can-i-import-urlparse-in-python-3")


# target_url = "http://192.168.44.101"
# target_links = []

# def extract_links_from(url):
# 	response = requests.get(url)
# 	return re.findall('(?:href=")(.*?)"',response.content)

# def crawl(url):
# 	href_links = extract_links_from(url)

# 	for link in href_links:
# 		link = urlparse.urljoin(url,link)

# 		if "#" in link:	# # refers to original page so avoid duplicate page again and again
# 			link = link.split("#")[0]

# 		if target_url in link and link not in target_links: #to avoid repeating the same url
# 			target_links.append(link)
# 			print("[+]urls --->", link)
# 			crawl(link) #recursively crawling

# def lunch(url):
# 	crawl(url)
